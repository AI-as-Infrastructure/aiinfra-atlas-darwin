# ATLAS Environment Configuration 

# ATLAS Version
ATLAS_VERSION="0.0.0"
LAST_MODIFIED="January 1901"

# Vue Frontend
VITE_SITE_TITLE="ATLAS Hansard"
VITE_API_URL=https://localhost/api

#Python version for virtual environment
PYTHON_VERSION="3.10"

# Worker Configuration and simultaneous LLM API calls, defaults set for server with 16 cores, 32GB RAM. Configured to support 20 concurrent users.
GUNICORN_WORKERS=6
LLM_THREAD_POOL_WORKERS=10

# SSL.
CERTS_PATH=dev_certs

# Environment toggle
ENVIRONMENT=development
# Set the debug level for the frontend: silent, error, warn, info, debug
VITE_LOG_LEVEL=debug
# Set the debug level for the backend: silent, error, warn, info, debug
BACKEND_LOG_LEVEL=debug

# System-wide telemetry control (true/false). When false, no telemetry data is collected from any user.
# This overrides all user privacy settings and completely disables telemetry to Phoenix Arize.
TELEMETRY_ENABLED=true

# UI privacy toggle visibility (true/false). When true, users see a Privacy toggle to control their own telemetry.
# When false, the Privacy toggle is hidden and all users' telemetry follows the system-wide setting.
VITE_TELEMETRY_ENABLED=true

# Toggle on and off (true / false) cognito authentication. If true make sure settings below are correct.
VITE_USE_COGNITO_AUTH=false

# Toggle on and off (true / false) feedback types (shows / hides buttons)
VITE_FEEDBACK_SIMPLE_ENABLED=true
VITE_FEEDBACK_ENHANCED_ENABLED=true
VITE_FEEDBACK_AI_ASSISTED_ENABLED=true
VITE_FEEDBACK_SKIP_ENABLED=true

# Only used for staging and prod, as a message queue for concurrent users
REDIS_PASSWORD="<DEFAULT>"
REDIS_URL=redis://:${REDIS_PASSWORD}@localhost:6379/1

# Define the test target, which will be imported into the main retriever file. Alternative targets can be produced, using blert_500 as a template. Note that there are important dependencies between the vector store, retriever, and TEST_TARGET. Test targets are stored in backend/targets.
TEST_TARGET=k40_claude4
# Set the main retriever. It's used in app.py to connect to LLMs using Langchain. Alternative retrievers can be produced for new vector stores. They are stored in the retrievers directory.
RETRIEVER_MODULE=hansard_retriever
# Controls how many documents are initially retrieved during HNSW vector search when a single corpus is selected. Note that this is different to the fetch-k (SEARCH_K) defined in TEST_TARGET, which defines the amount of chunks (pre-filtered via HNSW vector search) that get analysed by the LLM in its context window.
LARGE_RETRIEVAL_SIZE_SINGLE_CORPUS=500
# Controls how many documents are initially retrieved for each corpus during HNSW vector search when the ALL filter is chosen. Note that this is different to the fetch-k (SEARCH_K) defined in TEST_TARGET, which defines the amount of chunks (pre-filtered via HNSW vector search) that get analysed by the LLM in its context window.
LARGE_RETRIEVAL_SIZE_ALL_CORPUS=200
################################################################################
# Embedding model used when building / loading the Chroma vector store.
# • The store-builder (make store / make xml-store) downloads and converts this
#   model to Sentence-Transformers format automatically.
# • HansardRetriever loads exactly the same local path recorded in the store’s
#   manifest, so every environment must point at the *same* HF repo string.
# • Change the value to build a new store with a different model.
################################################################################
EMBEDDING_MODEL=Livingwithmachines/bert_1890_1900

# Determine whether a corpus selector dropdown will appear next to the question box. If True this requires that your vector store includes metadata tags matching the different corpuses in yourvector store. Refer to the docs for more information.
MULTI_CORPUS_VECTORSTORE=True
MULTI_CORPUS_METADATA="1901_au,1901_nz,1901_uk"

# Chroma Vector Store Configuration
CHROMA_PERSIST_DIRECTORY="backend/targets/chroma_db"
CHROMA_COLLECTION_NAME="blert_1000"

# Ollama configuration
OLLAMA_ENDPOINT="http://localhost:11434"

# LLM keys 
OPENAI_API_KEY="<DEFAULT>"
ANTHROPIC_API_KEY="<DEFAULT>"
GOOGLE_API_KEY="<DEFAULT>"

# Session Validation Configuration
VALIDATION_LLM_MODE=alternate           # "default" or "alternate"
VALIDATION_LLM_DEFAULT=gpt-4o           # LLM used when mode=default
VALIDATION_LLM_ALTERNATE=claude-3-5-sonnet-20241022  # LLM used when mode=alternate
VALIDATION_PROVIDER_DEFAULT=OPENAI      # Provider for default validation
VALIDATION_PROVIDER_ALTERNATE=ANTHROPIC  # Provider for alternate validation
VALIDATION_ENABLED=true                 # Enable/disable validation feature

# AWS Bedrock Configuration
AWS_DEFAULT_REGION=us-east-1
# Leave empty to use AWS CLI, IAM roles, or other credential providers
AWS_ACCESS_KEY_ID="<DEFAULT>"
AWS_SECRET_ACCESS_KEY="<DEFAULT>"

# Observability & monitoring
OTEL_EXPORTER_OTLP_HEADERS="api_key=<DEFAULT>"
PHOENIX_CLIENT_HEADERS="api_key=<DEFAULT>"
PHOENIX_PROJECT_NAME=ATLAS-Hansard-Dev
PHOENIX_COLLECTOR_ENDPOINT="https://app.phoenix.arize.com"
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_RESOURCE_ATTRIBUTES="service.name=atlas"

# AWS Cognito Config
VITE_USE_COGNITO_AUTH=false
VITE_COGNITO_REGION="<DEFAULT>"
VITE_COGNITO_DOMAIN="<DEFAULT>"
VITE_COGNITO_USERPOOL_ID="<DEFAULT>"
VITE_COGNITO_CLIENT_ID="<DEFAULT>"
VITE_COGNITO_REDIRECT_URI="<DEFAULT>"
VITE_COGNITO_LOGOUT_URL="<DEFAULT>"
VITE_COGNITO_OAUTH_SCOPE="openid email profile"

# WebSocket Configuration - Production settings for concurrent users
WEBSOCKET_MAX_CONNECTIONS=100
WEBSOCKET_MAX_IDLE_TIME=1800
WEBSOCKET_MAX_CONNECTION_TIME=7200
WEBSOCKET_MAX_MESSAGES=1000
WEBSOCKET_CLEANUP_INTERVAL=300
WEBSOCKET_MEMORY_THRESHOLD_MB=500

# LLM Memory Management Configuration
# Template defaults for realistic concurrent user capacity
LLM_MAX_CONCURRENT=20
LLM_MAX_RESPONSE_TOKENS=4000
LLM_MAX_RESPONSE_CHARS=32000

# TPM Rate Limiting Configuration - Optimized for realistic user behavior
LLM_REQUEST_DELAY_MS=1000       # 1s delay between requests (prevents API hammering)
LLM_RETRY_DELAY_MS=8000         # 8s delay before retrying TPM-limited requests
LLM_MAX_RETRIES=2               # Max retries for rate-limited requests (reduced)

# Prompt Caching Configuration (Anthropic Claude only)
PROMPT_CACHING_ENABLED=true     # Enable prompt caching for supported models
PROMPT_CACHE_TTL=5m             # Cache TTL: 5m or 1h (1h requires beta header)
PROMPT_CACHE_SYSTEM=true        # Cache system prompts
PROMPT_CACHE_CONTEXT=true       # Cache document context

# Gunicorn Worker Configuration
# 8 workers for 8 vCPU cores, with production-grade memory limits
GUNICORN_WORKERS=8
GUNICORN_MAX_REQUESTS=3000
GUNICORN_MAX_REQUESTS_JITTER=300
GUNICORN_TIMEOUT=300
GUNICORN_KEEPALIVE=30
GUNICORN_MAX_WORKER_MEMORY_MB=1800

# FastAPI Request Limits
# Production-grade limits for handling larger requests
FASTAPI_MAX_REQUEST_SIZE_MB=20
FASTAPI_MAX_RESPONSE_SIZE_MB=100

# Memory Management
# Chat history limits per session
CHAT_HISTORY_MAX_MESSAGES=100
CHAT_HISTORY_MAX_CONTENT_SIZE_MB=1

# Redis Configuration for concurrent users
REDIS_MAX_CONNECTIONS=100
REDIS_MAX_MEMORY_MB=1024

# Rate Limiting Configuration
# Rate limiting to prevent abuse while allowing normal research usage
RATE_LIMIT_PER_MINUTE=240


################################################################################
# Vector store creation
################################################################################
# Embedding model used when building / loading the Chroma vector store. The store-builder (make store / make xml-store) downloads and converts this model to Sentence-Transformers format automatically. HansardRetriever loads exactly the same local path recorded in the store’s manifest, so every environment must point at the *same* HF repo string.Change the value to build a new store with a different model.
EMBEDDING_MODEL=Livingwithmachines/bert_1890_1900 
# Embedding pooling strategy. The vector store can be built (and later queried) with different token-pooling schemes.  Set the desired mode here. The default mode is designed to maximise performance of Livingwithmachines/bert_1890_1900
# Values = mean, cls, or mean+max
POOLING=mean+max
CHUNK_SIZE=1000
CHUNK_OVERLAP=100
TEXT_SPLITTER_TYPE=RecursiveCharacterTextSplitter



